from enum import Enum
import os
import asyncio
import traceback
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.agents import AssistantAgent
from typing import Any, Awaitable, Callable, List, Mapping, Sequence, AsyncGenerator, Union, Optional, Tuple, Dict
from pydantic import BaseModel

from autogen_agentchat.messages import (
    BaseChatMessage, 
    BaseAgentEvent, 
    TextMessage,
    ModelClientStreamingChunkEvent, 
    StopMessage, 
    MemoryQueryEvent, 
    ToolCallExecutionEvent, 
    ToolCallSummaryMessage,
    ToolCallRequestEvent,
    ThoughtEvent,
    StructuredMessage,
    
)
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.base import ChatAgent, TaskResult, Team, TerminationCondition, Response
from autogen_agentchat.base import Handoff as HandoffBase
from autogen_agentchat.conditions import TextMentionTermination, ExternalTermination
from autogen_core import CancellationToken, FunctionCall
from autogen_core.models import ChatCompletionClient, CreateResult
from autogen_core.tools import BaseTool
from autogen_core.memory import Memory, ListMemory, MemoryContent
from autogen_core import CancellationToken, ComponentBase, trace_create_agent_span, trace_invoke_agent_span
from autogen_core import CancellationToken
from autogen_core.model_context import UnboundedChatCompletionContext, ChatCompletionContext
from autogen_core.models import (
    FunctionExecutionResult,
    LLMMessage,
    RequestUsage,
    UserMessage,
    SystemMessage,
    AssistantMessage,
    FunctionExecutionResultMessage,
)
from autogen_core.tools import BaseTool, Workbench
from rich.console import Console as RichConsole

from ..session.session_state_manager import SessionStateManager, SessionStateStatus
from .agent_param import AgentParam
# å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜ - æ”¯æŒç›´æ¥è¿è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥
try:
    from .memory_recorder import MemoryRecorder
except ImportError:
    from wr124.agents.memory_recorder import MemoryRecorder

KEYWORD = "_I_HAVE_COMPLETED_"

STOP_PROMPT = f'''

# åœæ­¢æ¡ä»¶
**This is VERY important**
When the assigned tasks are completed and there is no other work to execute, output the termination keyword.
the termination keyword is: `{KEYWORD}`

'''
NOTE_PROMPT = f'''
This is a note: It is not part of the task but can guide your work.
1. Remembering your task goals or to-do goals is very important as it can guide your direction and prevent you from deviating.
2. When you are unsure about the next step, you can use `todo_read` or `list_tasks` to understand the task progress.
3. After completing a task or to-do, remember to update the task status.
4. Once all tasks are completed, enter the termination keyword. Note: Only output it when all tasks are finished; otherwise, continue executing tasks.
5. if you encounter any issues or blockers, using the `search_agent` tool to look up internet resources can be a very effective way to address them.
'''

class STOP_REASON(str, Enum):
    COMPLETED = "ä»»åŠ¡æˆåŠŸå®Œæˆ"
    CANCELLED = "ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ"
    TIMEOUT = "ä»»åŠ¡è¶…æ—¶"
    ERROR = "ä»»åŠ¡æ‰§è¡Œå‡ºé”™"
    MAX_ITERATIONS = "è¾¾åˆ°æ¬¡æ•°ä¸Šé™"
    EXTERNAL_TERMINATION = "å¤–éƒ¨ç»ˆæ­¢ä¿¡å·"
    EXIT = "ç”¨æˆ·ä¸»åŠ¨é€€å‡º"
    INVALID_TASK = "æ— æ•ˆä»»åŠ¡"
    UNKNOWN = "æœªçŸ¥åŸå› "


class NoSystemUnboundedChatCompletionContext(UnboundedChatCompletionContext):
    """ä¸åŒ…å«ç³»ç»Ÿæ¶ˆæ¯çš„æ— ç•ŒèŠå¤©ä¸Šä¸‹æ–‡"""
    def remove_system_messages(self) -> None:
        """ç§»é™¤ç³»ç»Ÿæ¶ˆæ¯"""
        self._messages = [msg for msg in self._messages if not isinstance(msg, SystemMessage)]

class BaseAgent(AssistantAgent):
    component_provider_override = "BaseAgent"
    _max_tokens_for_process = 40000  # ç±»å˜é‡ï¼Œç”¨äº _process_model_result æ–¹æ³•
    
    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        description: str = "",
        system_message: str = f"you are a helpful assistant, completing tasks as requested. {STOP_PROMPT}",
        tools: List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None = None,
        reflect_on_tool_use: bool | None = None,
        memory: Sequence[Memory] | None = None,
        enable_memory_recording: bool = False,
        max_tool_iterations=40,
        max_tokens: int = 40000,
        max_compress_count: Optional[int] = None,
        hook_agents: Optional[List[AgentParam]] = None,
        compress_agent: Optional[AgentParam] = None,
        **kwargs,
    ) -> None:
       
        note = MemoryContent(content=NOTE_PROMPT, mime_type="text/plain")
        note_memory = ListMemory(memory_contents=[note])
        if memory:
            if isinstance(memory, list):  # ç¡®ä¿ memory æ˜¯ List ç±»å‹
                memory.append(note_memory)
            else:
                memory = list(memory) + [note_memory]  # è½¬æ¢ä¸º List å¹¶æ·»åŠ å…ƒç´ 
        else:
            memory = [note_memory]
        if isinstance(model_client, AnthropicChatCompletionClient):
            memory = None
        
        super().__init__(
            name,
            model_client,
            model_context=NoSystemUnboundedChatCompletionContext(),
            description=description,
            system_message=system_message,
            tools=tools,
            reflect_on_tool_use=reflect_on_tool_use,
            memory=memory,   
            max_tool_iterations=max_tool_iterations,         
            **kwargs,
        )
        self._temrminate_word = KEYWORD
        self._termination_condition = TextMentionTermination(self._temrminate_word)
        self._model_client = model_client
        self._max_tokens = max_tokens   # token
        # è®¾ç½®ç±»å˜é‡ï¼Œä¾› _process_model_result æ–¹æ³•ä½¿ç”¨
        BaseAgent._max_tokens_for_process = max_tokens
        self._min_tool_count_to_summary = 20


        self._max_compress_count = max_compress_count if max_compress_count is not None else 0  # å‹ç¼©æ¬¡æ•°
         # å‹ç¼©agent
        self._compress_agent_param = compress_agent
               
        # Rich console for beautiful output
        self._hook_agents = hook_agents if hook_agents is not None else []

        self._console = RichConsole()
        
        # è®°å¿†è®°å½•åŠŸèƒ½
        self._enable_memory_recording = enable_memory_recording
        self._memory_recorder: Optional[MemoryRecorder] = None
        self._memory_queue: Optional[asyncio.Queue] = None
        self._memory_task: Optional[asyncio.Task] = None
        
        if self._enable_memory_recording:
            self._memory_recorder = MemoryRecorder(model_client, name)
            self._memory_queue = asyncio.Queue(maxsize=100)  # é™åˆ¶é˜Ÿåˆ—å¤§å°
        self._session_manager: Optional[SessionStateManager] = None

    def register_session_manager(self, session_manager: SessionStateManager):
        self._session_manager = session_manager

    @property
    def tools(self):
        return self._tools

    async def run(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> TaskResult:
        result: TaskResult | None = None
        async for message in self.run_stream(
            task=task,
            cancellation_token=cancellation_token,
            output_task_messages=output_task_messages,
        ):
            if isinstance(message, TaskResult):
                result = message
        if result is not None:
            return result
        raise AssertionError("The stream should have returned the final result.")

    async def run_stream(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
    
        with trace_invoke_agent_span(agent_name=self.name, agent_description=self.description):
            if cancellation_token is None:
                cancellation_token = CancellationToken()
            
            # å¯åŠ¨è®°å¿†è®°å½•ä»»åŠ¡
            if self._enable_memory_recording and self._memory_recorder and self._memory_queue:
                self._memory_task = asyncio.create_task(
                    self._memory_recorder.start_recording(self._memory_queue, cancellation_token)
                )
            
            try:
                input_messages: List[BaseChatMessage] = []
                output_messages: List[BaseAgentEvent | BaseChatMessage] = []
                if task is None:
                    pass
                elif isinstance(task, str):
                    text_msg = TextMessage(content=task, source="user")
                    input_messages.append(text_msg)
                    if output_task_messages:
                        output_messages.append(text_msg)
                        yield text_msg
                        # å‘é€åˆ°è®°å¿†é˜Ÿåˆ—
                        self._add_to_memory_queue(text_msg)
                elif isinstance(task, BaseChatMessage):
                    input_messages.append(task)
                    if output_task_messages:
                        output_messages.append(task)
                        yield task
                        # å‘é€åˆ°è®°å¿†é˜Ÿåˆ—
                        self._add_to_memory_queue(task)
                else:
                    if not task:
                        raise ValueError("Task list cannot be empty.")
                    for msg in task:
                        if isinstance(msg, BaseChatMessage):
                            input_messages.append(msg)
                            if output_task_messages:
                                output_messages.append(msg)
                                yield msg
                                # å‘é€åˆ°è®°å¿†é˜Ÿåˆ—
                                self._add_to_memory_queue(msg)
                        else:
                            raise ValueError(f"Invalid message type in sequence: {type(msg)}")
                input_messages_bak = input_messages.copy()
                models_usage = RequestUsage(0,0)
                stop_reason: str = STOP_REASON.UNKNOWN
                completed = False
                trigger_summary = False
                skip_stop = False  # ç»“æŸå…³é”®è¯è·³è¿‡
                compress_count = 0  # å‹ç¼©ä¸Šä¸‹æ–‡æ¬¡æ•°ï¼Œå‹ç¼©ä¸Šä¸‹æ–‡å¯ä»¥å‡å°‘tokenä½¿ç”¨ï¼Œç›¸å½“äºé‡å¯ä»»åŠ¡ï¼ˆä»…ä¿ç•™å°‘æ•°æ€»ç»“ä¿¡æ¯ï¼‰ã€‚è¶…è¿‡å›ºå®šæ¬¡æ•°ï¼Œä»»åŠ¡è¿˜æœªå®Œæˆï¼Œåˆ™é€€å‡ºã€‚éœ€è¦äººå·¥ä»‹å…¥è§£å†³å¤æ‚éš¾é¢˜
                tool_count = 0  # ç»Ÿè®¡ä¸€æ¬¡æ€§è°ƒç”¨å·¥å…·æ¬¡æ•°ï¼Œå·¥å…·è°ƒç”¨æ¬¡æ•°å¤§äº self._min_tool_count_to_summaryï¼Œè¿›è¡Œä¸€æ¬¡æ€»ç»“ï¼Œè§„åˆ’ä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œä¸è¶³æ—¶ä¸åšå¤„ç†
                while True:
                    if cancellation_token.is_cancelled():
                        stop_reason = STOP_REASON.CANCELLED
                        break
                    if self._termination_condition.terminated or completed:
                        break

                    models_usage = RequestUsage(0,0)
                    async for message in self.on_messages_stream(input_messages, cancellation_token):
                    
                        if isinstance(message, Response):
                            if trigger_summary:
                                trigger_summary = False
                                skip_stop = True # å¦‚æœä¸Šæ¬¡æ˜¯summaryæç¤ºï¼Œåˆ™è·³è¿‡ç»ˆæ­¢æ¡ä»¶æ£€æŸ¥ï¼ˆå› ä¸ºæ€»ç»“è¿‡ç¨‹ä¸­å¯èƒ½è¾“å‡º _I_HAVE_COMPLETED_ å­—æ ·ï¼‰ï¼Œè¿™ä¸ªä¸æ˜¯æœŸæœ›çš„ï¼Œ å…¶ä»–æ—¶å€™éƒ½ä¿æŒFalse
                            # ç»Ÿè®¡tokenä½¿ç”¨æƒ…å†µ
                            if message.chat_message.models_usage and message.chat_message.models_usage.prompt_tokens > models_usage.prompt_tokens:                                      
                                models_usage = message.chat_message.models_usage

                            yield message.chat_message                            
                            output_messages.append(message.chat_message)
                            # å‘é€åˆ°è®°å¿†é˜Ÿåˆ—
                            self._add_to_memory_queue(message.chat_message)
                            if isinstance(message.chat_message, ToolCallSummaryMessage):
                                # å½“max_tool_iterationsè®¾ç½®>1 æ—¶ï¼Œå¤šæ¬¡çš„å·¥å…·è°ƒç”¨åï¼Œåšä¾æ¬¡æ€»ç»“æ˜¯æœ‰å¿…è¦çš„ã€‚
                                if tool_count >= self._min_tool_count_to_summary:
                                    input_messages = [TextMessage(content="å…ˆæ€»ç»“ä»¥ä¸Šå·¥å…·è°ƒç”¨ç»“æœï¼Œå½¢æˆé˜¶æ®µæ€§åˆ†æç»“è®º. å†æè¿°åç»­é¡»æ‰§è¡Œçš„åŠ¨ä½œä»¥æŒ‡å¯¼æ¨è¿›ä»»åŠ¡ç›®æ ‡å®Œæˆ", source='user')]
                                    trigger_summary = True
                                else:
                                    input_messages = []
                            else:
                                input_messages = []
                            tool_count = 0

                            
                            # æ˜¯å¦è¦è·³è¿‡ä¸€æ¬¡åˆ¤æ–­
                            if skip_stop:
                                skip_stop = False
                            else:
                                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»ˆæ­¢æ¡ä»¶
                                stop_message = await self._termination_condition([message.chat_message])                            
                                if stop_message is not None:
                                    # Reset the termination conditions and turn count.
                                    await self._termination_condition.reset()
                                    completed = True
                                    stop_reason=STOP_REASON.COMPLETED
                                    break

                        else:
                            if message.models_usage and message.models_usage.prompt_tokens > models_usage.prompt_tokens:                                      
                                models_usage = message.models_usage

                            yield message
                            if isinstance(message, ModelClientStreamingChunkEvent):
                                # Skip the model client streaming chunk events.
                                continue
                            if isinstance(message,ToolCallRequestEvent):
                                tool_count += 1
                            output_messages.append(message)
                            # å‘é€åˆ°è®°å¿†é˜Ÿåˆ—
                            self._add_to_memory_queue(message)

                    # å¦‚æœoutput_messages è®¡ç®—tokenæ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œåˆ™éœ€è¦è¿›è¡Œæ‘˜è¦ï¼Œå¹¶å°†æ‘˜è¦ä½œä¸ºæ–°çš„è¾“å…¥
                    if models_usage.prompt_tokens > self._max_tokens and not completed:
                        compress_count += 1
                        if compress_count > self._max_compress_count:
                            self._console.print(f"[yellow]âš ï¸  tokenå‹ç¼©æ¬¡æ•°è¶…è¿‡ä¸Šé™{self._max_compress_count}ï¼Œåœæ­¢[/yellow]")
                            stop_reason = STOP_REASON.MAX_ITERATIONS

                            break
                        summary = await self._compress_message(cancellation_token)
                        input_messages = input_messages_bak + summary
                        models_usage = RequestUsage(0,0)
                        await self.upload_state("compress history")                                                
                        await self._model_context.clear() # æ¸…ç©ºæ¨¡å‹ä¸Šä¸‹æ–‡ï¼Œä»¥ä¾¿è¿›è¡Œæ–°çš„è¾“å…¥    

                    # å·¥å…·æ™ºèƒ½ä½“æ¯æ¬¡ä½¿ç”¨ååˆå§‹åŒ–          
                    try:
                        for tool in self._tools:
                            if isinstance(tool, AgentTool):
                                await tool._agent.on_reset(cancellation_token)  # é‡ç½®å·¥å…·çŠ¶æ€ï¼Œé¿å…å·¥å…·å†…å­˜è¿‡å¤§
                    except Exception as e:
                        self._console.print(f"[red]âš ï¸  é‡ç½®å·¥å…·çŠ¶æ€æ—¶å‡ºé”™: {e}[/red]")
                if stop_reason == STOP_REASON.COMPLETED or stop_reason == STOP_REASON.MAX_ITERATIONS:
                    hook_messages = await self._hook_agents_run(cancellation_token)
                    output_messages.extend(hook_messages)

                yield TaskResult(messages=output_messages, stop_reason=stop_reason)                    
            finally:
                await self._cleanup_memory_task()
                


    async def on_messages_stream(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
                                 )-> AsyncGenerator[Union[BaseAgentEvent, BaseChatMessage, Response], None]:
        """
        é‡è½½åº•å±‚AssitantAgentçš„on_messages_streamæ–¹æ³•, ä¸»è¦æ·»åŠ å·¥å…·è°ƒç”¨å¤±è´¥åé‡è¯•åŠŸèƒ½
        1. å¤„ç†æ¶ˆæ¯æµï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
        2. æœ€å¤šé‡è¯•5æ¬¡ï¼Œæ¯æ¬¡é—´éš”é€’å¢çš„ç­‰å¾…æ—¶é—´
        """
        max_retries = 3
        retry_delay = 1  # åˆå§‹å»¶è¿Ÿ2ç§’
        
        attempt = 0
        while(attempt <= max_retries):       
            try:
                async for message in super().on_messages_stream(messages, cancellation_token):
                    if isinstance(message, MemoryQueryEvent) :
                        continue
                    if isinstance(message, ToolCallExecutionEvent):
                        attempt = 0
                        continue
                    yield message
                # å¦‚æœæˆåŠŸå¤„ç†å®Œæ‰€æœ‰æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
                # self._model_contextä¸­æ·»åŠ è¿›å»çš„SystemMessageéƒ½ç»™è¸¢å‡ºæ¥ã€‚é¿å…SystemMessageåœ¨æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­é‡å¤æ·»åŠ 
                if isinstance(self._model_context, NoSystemUnboundedChatCompletionContext):
                    self._model_context.remove_system_messages()
                    
                return               
            except asyncio.CancelledError:
                # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†               
                raise
            
            except Exception as e:
                attempt += 1
                # æ£€æŸ¥æ˜¯å¦æ˜¯MCPæµå¼è°ƒç”¨ç›¸å…³çš„å¼‚å¸¸ï¼ˆé€šå¸¸æ¥è‡ªanyioåº“ï¼‰
                exception_name = type(e).__name__
                if exception_name in ['BrokenResourceError', 'ClosedResourceError']:
                    # è¿™äº›å¼‚å¸¸é€šå¸¸è¡¨ç¤ºæµè¢«ä¸­æ–­ï¼Œå¯èƒ½æ˜¯ç”±äºESCé”®ä¸­æ–­
                    self._console.print(f"[yellow][{self.name}] MCP stream interrupted ({exception_name}), handling gracefully...[/yellow]")
                    # å¦‚æœæ˜¯å–æ¶ˆå¼•èµ·çš„ï¼Œç›´æ¥è¿”å›
                    if cancellation_token and cancellation_token.is_cancelled():
                        self._console.print(f"[cyan][{self.name}] Task was cancelled, stopping gracefully.[/cyan]")
                        return
                    # å¯¹äºæµä¸­æ–­å¼‚å¸¸ï¼Œä¸é‡è¯•ï¼Œç›´æ¥è¿”å›
                    self._console.print(f"[yellow][{self.name}] Stream interrupted, ending task execution.[/yellow]")
                    return
                
                error_detail = traceback.format_exc()
                content = f"é‡åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œè¯·ç¡®è®¤å·¥å…·è°ƒç”¨å‚æ•°æ ¼å¼éƒ½æ­£ç¡®ã€‚é—®é¢˜å¦‚ä¸‹:\n{str(e)}\n{error_detail}"
                messages = [TextMessage(content=content, source='user')]

                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if attempt >= max_retries:
                    final_error_msg = f"[{self.name}] Failed after {max_retries + 1} attempts. Final error: {type(e).__name__}: {str(e)}\n{error_detail}"
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
                    raise Exception(final_error_msg) from e
                
                # è®¡ç®—ä¸‹æ¬¡é‡è¯•çš„å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                current_delay = retry_delay * (2 ** attempt)
                self._console.print(f"[{self.name}] Retrying in {current_delay} seconds...", style="red")
                await asyncio.sleep(current_delay)
                if cancellation_token and cancellation_token.is_cancelled():
                    return


    async def _hook_agents_run(self, cancellation_token: CancellationToken) -> List[BaseChatMessage]:
        """è¿è¡ŒæŒ‚é’©æ™ºèƒ½ä½“ï¼Œè¿”å›å®ƒä»¬çš„è¾“å‡ºæ¶ˆæ¯åˆ—è¡¨"""
        all_hook_messages: List[BaseChatMessage] = []
        for agent_param in self._hook_agents:
            if agent_param.task is None:
                self._console.print(f"[yellow]âš ï¸  æŒ‚é’©æ™ºèƒ½ä½“ {agent_param.name} æœªé…ç½®ä»»åŠ¡ï¼Œè·³è¿‡[/yellow]")
                continue
            try:

                filtered_tools = []
                
                for tool_name in agent_param.tools:
                    for tool in self._tools:
                        if (hasattr(tool, 'name') and tool.name == tool_name) or \
                            (hasattr(tool, '__name__') and tool.__name__ == tool_name) or \
                            (hasattr(tool, 'schema') and tool.schema.get('name') == tool_name):
                            filtered_tools.append(tool)
                            break

                hook_agent = AssistantAgent(
                    name=agent_param.name,
                    model_client=self._model_client,
                    description=agent_param.description,
                    system_message=agent_param.prompt,
                    model_context=self._model_context,
                    tools=filtered_tools,
                    reflect_on_tool_use=False,
                    max_tool_iterations=agent_param.max_tool_iterations if agent_param.max_tool_iterations is not None else 5,
                )
                self._console.print(f"[cyan]ğŸ¤– Running hook agent: {agent_param.name}[/cyan]")
                response = await hook_agent.on_messages(messages=[TextMessage(content=agent_param.task,source='user')], cancellation_token=cancellation_token)
                all_hook_messages.append(response.chat_message)
            except Exception as e:
                self._console.print(f"[red]Error running hook agent {agent_param.name}: {e}[/red]")
        return all_hook_messages



    async def _compress_message(self, cancellation_token: CancellationToken | None = None,) -> List[BaseChatMessage]:
        if self._compress_agent_param is None:
            self._console.print(f"[yellow]âš ï¸  å‹ç¼©Agentæœªé…ç½®ï¼Œæ— æ³•å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡[/yellow]")
            return []

        if cancellation_token is None:
            cancellation_token = CancellationToken()

        filtered_tools = []
        
        for tool_name in self._compress_agent_param.tools:
            for tool in self._tools:
                if (hasattr(tool, 'name') and tool.name == tool_name) or \
                    (hasattr(tool, '__name__') and tool.__name__ == tool_name) or \
                    (hasattr(tool, 'schema') and tool.schema.get('name') == tool_name):
                    filtered_tools.append(tool)
                    break
        if len(filtered_tools) != len(self._compress_agent_param.tools):
            self._console.print(f"[yellow]âš ï¸  éƒ¨åˆ†å‹ç¼©Agentå·¥å…·æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚æœŸæœ›å·¥å…·: {self._compress_agent_param.tools}, å®é™…å·¥å…·: {[tool.name if hasattr(tool, 'name') else (tool.__name__ if hasattr(tool, '__name__') else tool.schema.get('name') if hasattr(tool, 'schema') else str(tool)) for tool in filtered_tools]}[/yellow]")

        compress_agent = AssistantAgent(
            name=f'{self.name}_compressor',
            model_client=self._model_client,
            description=self._compress_agent_param.description,
            system_message=self._compress_agent_param.prompt,
            model_context=self._model_context,
            tools=filtered_tools,
            max_tool_iterations=self._compress_agent_param.max_tool_iterations if self._compress_agent_param.max_tool_iterations is not None else 5,
        )
        msg = TextMessage(
            content=self._compress_agent_param.task if self._compress_agent_param.task else "Please summarize the conversation following system prompt. first call `add_memory` to upload summary to database. add then output the summary to user",
            source="user",
        )

        res:Response = await compress_agent.on_messages([msg], cancellation_token)
        self._add_to_memory_queue(res.chat_message)
        summary = [res.chat_message]
        return summary

    def _add_to_memory_queue(self, message: BaseChatMessage | BaseAgentEvent) -> None:
        """å°†æ¶ˆæ¯æ·»åŠ åˆ°è®°å¿†é˜Ÿåˆ—"""
        if not self._enable_memory_recording or not self._memory_queue:
            return
        
        try:
            self._memory_queue.put_nowait(message)
        except asyncio.QueueFull:
            # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€è€çš„æ¶ˆæ¯
            try:
                self._memory_queue.get_nowait()
                self._memory_queue.put_nowait(message)
            except asyncio.QueueEmpty:
                pass

    async def _cleanup_memory_task(self) -> None:
        """æ¸…ç†è®°å¿†ä»»åŠ¡"""
        if not self._memory_task or not self._memory_queue:
            return
        
        # å‘é€ç»“æŸä¿¡å·
        try:
            self._memory_queue.put_nowait(None)
        except asyncio.QueueFull:
            pass
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
        try:
            await asyncio.wait_for(self._memory_task, timeout=3.0)
        except asyncio.TimeoutError:
            # è¶…æ—¶åˆ™å–æ¶ˆä»»åŠ¡
            self._memory_task.cancel()
            try:
                await self._memory_task
            except asyncio.CancelledError:
                pass

    async def upload_state(self, note: str):
        if self._session_manager:
            msgs = await self._model_context.get_messages()
            if len(msgs) < 10:
                return
            state = await self.save_state()
            await self._session_manager.upload_session_state(self.name, None, state, note)

    async def download_state(self):
        if self._session_manager:
            ret, state = await self._session_manager.restore_agent_session_state(self.name)
            if ret == SessionStateStatus.SUCCESS:
                if isinstance(state, dict):
                    await self.load_state(state)
                return
            else:
                ret, state = await self._session_manager.restore_latest_session_state(self.name)
                if ret == SessionStateStatus.SUCCESS:
                    if isinstance(state, dict):
                        await self.load_state(state)
                    return
                else:
                    return

    @classmethod
    async def _process_model_result(
        cls,
        model_result: CreateResult,
        inner_messages: List[BaseAgentEvent | BaseChatMessage],
        cancellation_token: CancellationToken,
        agent_name: str,
        system_messages: List[SystemMessage],
        model_context: ChatCompletionContext,
        workbench: Sequence[Workbench],
        handoff_tools: List[BaseTool[Any, Any]],
        handoffs: Dict[str, HandoffBase],
        model_client: ChatCompletionClient,
        model_client_stream: bool,
        reflect_on_tool_use: bool,
        tool_call_summary_format: str,
        tool_call_summary_formatter: Callable[[FunctionCall, FunctionExecutionResult], str] | None,
        max_tool_iterations: int,
        output_content_type: type[BaseModel] | None,
        message_id: str,
        format_string: str | None = None,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        """
        Handle final or partial responses from model_result, including tool calls, handoffs,
        and reflection if needed. Supports tool call loops when enabled.
        """

        # Tool call loop implementation with streaming support
        current_model_result = model_result
        # This variable is needed for the final summary/reflection step
        executed_calls_and_results: List[Tuple[FunctionCall, FunctionExecutionResult]] = []

        for loop_iteration in range(max_tool_iterations):
            # If direct text response (string), we're done
            if isinstance(current_model_result.content, str):
                # Use the passed message ID for the final message
                if output_content_type:
                    content = output_content_type.model_validate_json(current_model_result.content)
                    yield Response(
                        chat_message=StructuredMessage[output_content_type](  # type: ignore[valid-type]
                            content=content,
                            source=agent_name,
                            models_usage=current_model_result.usage,
                            format_string=format_string,
                            id=message_id,
                        ),
                        inner_messages=inner_messages,
                    )
                else:
                    yield Response(
                        chat_message=TextMessage(
                            content=current_model_result.content,
                            source=agent_name,
                            models_usage=current_model_result.usage,
                            id=message_id,
                        ),
                        inner_messages=inner_messages,
                    )
                return

            # Otherwise, we have function calls
            assert isinstance(current_model_result.content, list) and all(
                isinstance(item, FunctionCall) for item in current_model_result.content
            )

            # STEP 4A: Yield ToolCallRequestEvent
            tool_call_msg = ToolCallRequestEvent(
                content=current_model_result.content,
                source=agent_name,
                models_usage=current_model_result.usage,
            )

            inner_messages.append(tool_call_msg)
            yield tool_call_msg

            # STEP 4B: Execute tool calls with streaming support
            # Use a queue to handle streaming results from tool calls.
            stream = asyncio.Queue[BaseAgentEvent | BaseChatMessage | None]()

            async def _execute_tool_calls(
                function_calls: List[FunctionCall],
                stream_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | None],
            ) -> List[Tuple[FunctionCall, FunctionExecutionResult]]:
                results = await asyncio.gather(
                    *[
                        cls._execute_tool_call(
                            tool_call=call,
                            workbench=workbench,
                            handoff_tools=handoff_tools,
                            agent_name=agent_name,
                            cancellation_token=cancellation_token,
                            stream=stream_queue,
                        )
                        for call in function_calls
                    ]
                )
                # Signal the end of streaming by putting None in the queue.
                stream_queue.put_nowait(None)
                return results

            task = asyncio.create_task(_execute_tool_calls(current_model_result.content, stream))

            while True:
                event = await stream.get()
                if event is None:
                    # End of streaming, break the loop.
                    break
                if isinstance(event, BaseAgentEvent) or isinstance(event, BaseChatMessage):
                    yield event
                    inner_messages.append(event)
                else:
                    raise RuntimeError(f"Unexpected event type: {type(event)}")

            # Wait for all tool calls to complete.
            executed_calls_and_results = await task
            exec_results = [result for _, result in executed_calls_and_results]

            # Yield ToolCallExecutionEvent
            tool_call_result_msg = ToolCallExecutionEvent(
                content=exec_results,
                source=agent_name,
            )

            await model_context.add_message(FunctionExecutionResultMessage(content=exec_results))
            inner_messages.append(tool_call_result_msg)
            yield tool_call_result_msg

            # STEP 4C: Check for handoff
            handoff_output = cls._check_and_handle_handoff(
                model_result=current_model_result,
                executed_calls_and_results=executed_calls_and_results,
                inner_messages=inner_messages,
                handoffs=handoffs,
                agent_name=agent_name,
            )
            if handoff_output:
                yield handoff_output
                return

            # STEP 4D: Check if we should continue the loop.
            # If we are on the last iteration, break to the summary/reflection step.
            if loop_iteration == max_tool_iterations - 1:
                break
            print(current_model_result.usage.prompt_tokens, current_model_result.usage.completion_tokens)
            if current_model_result.usage.prompt_tokens > cls._max_tokens_for_process:
                print(f"âš ï¸  Token usage {current_model_result.usage.prompt_tokens} exceeds limit {cls._max_tokens_for_process}, stopping tool call loop.")
                break

            # Continue the loop: make another model call using _call_llm
            next_model_result: Optional[CreateResult] = None
            async for llm_output in cls._call_llm(
                model_client=model_client,
                model_client_stream=model_client_stream,
                system_messages=system_messages,
                model_context=model_context,
                workbench=workbench,
                handoff_tools=handoff_tools,
                agent_name=agent_name,
                cancellation_token=cancellation_token,
                output_content_type=output_content_type,
                message_id=message_id,  # Use same message ID for consistency
            ):
                if isinstance(llm_output, CreateResult):
                    next_model_result = llm_output
                else:
                    # Streaming chunk event
                    yield llm_output

            assert next_model_result is not None, "No model result was produced in tool call loop."
            current_model_result = next_model_result

            # Yield thought event if present
            if current_model_result.thought:
                thought_event = ThoughtEvent(content=current_model_result.thought, source=agent_name)
                yield thought_event
                inner_messages.append(thought_event)

            # Add the assistant message to the model context (including thought if present)
            await model_context.add_message(
                AssistantMessage(
                    content=current_model_result.content,
                    source=agent_name,
                    thought=getattr(current_model_result, "thought", None),
                )
            )

        # After the loop, reflect or summarize tool results
        if reflect_on_tool_use:
            async for reflection_response in cls._reflect_on_tool_use_flow(
                system_messages=system_messages,
                model_client=model_client,
                model_client_stream=model_client_stream,
                model_context=model_context,
                workbench=workbench,
                handoff_tools=handoff_tools,
                agent_name=agent_name,
                inner_messages=inner_messages,
                output_content_type=output_content_type,
                cancellation_token=cancellation_token,
            ):
                yield reflection_response
        else:
            yield cls._summarize_tool_use(
                executed_calls_and_results=executed_calls_and_results,
                inner_messages=inner_messages,
                handoffs=handoffs,
                tool_call_summary_format=tool_call_summary_format,
                tool_call_summary_formatter=tool_call_summary_formatter,
                agent_name=agent_name,
            )
        return
if __name__ == "__main__":
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.ui import Console
    from dotenv import load_dotenv
    import asyncio
    import os
    import sys
    from pathlib import Path
    from typing_extensions import Annotated
    load_dotenv()
    current_dir = os.getcwd()
    project_root = Path(current_dir).parent
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    sys.path.insert(0, str(current_dir))
    sys.path.insert(0, str(project_root))

    def test(param:Annotated[str, "param"]) ->str:
        return 'aaa'
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = BaseAgent(name="WorkerAgent", model_client=model_client, tools = [test])
    
    # Example usage
    async def main():
        await Console(agent.run_stream(task="What is the weather today?"))
        
    
    asyncio.run(main())